{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a directory if not exists\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n",
    "\n",
    "# Hyper-parameters\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 15\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='../data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "# Data Loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/15], Step [10/469], Reconst Loss: 37527.1797, KL Div: 2215.7207\n",
      "Epoch[1/15], Step [20/469], Reconst Loss: 29678.2109, KL Div: 1192.4722\n",
      "Epoch[1/15], Step [30/469], Reconst Loss: 27221.8828, KL Div: 1219.1658\n",
      "Epoch[1/15], Step [40/469], Reconst Loss: 26725.2773, KL Div: 679.0154\n",
      "Epoch[1/15], Step [50/469], Reconst Loss: 26753.5859, KL Div: 731.0175\n",
      "Epoch[1/15], Step [60/469], Reconst Loss: 24786.4688, KL Div: 939.4626\n",
      "Epoch[1/15], Step [70/469], Reconst Loss: 24787.0977, KL Div: 866.4673\n",
      "Epoch[1/15], Step [80/469], Reconst Loss: 24433.2773, KL Div: 864.0120\n",
      "Epoch[1/15], Step [90/469], Reconst Loss: 23825.1484, KL Div: 1043.4161\n",
      "Epoch[1/15], Step [100/469], Reconst Loss: 23211.9180, KL Div: 1318.6299\n",
      "Epoch[1/15], Step [110/469], Reconst Loss: 21335.8711, KL Div: 1416.9479\n",
      "Epoch[1/15], Step [120/469], Reconst Loss: 20779.1777, KL Div: 1731.3557\n",
      "Epoch[1/15], Step [130/469], Reconst Loss: 19461.7969, KL Div: 1603.1835\n",
      "Epoch[1/15], Step [140/469], Reconst Loss: 19236.0273, KL Div: 1747.5487\n",
      "Epoch[1/15], Step [150/469], Reconst Loss: 19243.6406, KL Div: 1952.9442\n",
      "Epoch[1/15], Step [160/469], Reconst Loss: 19994.0547, KL Div: 1707.6005\n",
      "Epoch[1/15], Step [170/469], Reconst Loss: 18868.5547, KL Div: 1989.2719\n",
      "Epoch[1/15], Step [180/469], Reconst Loss: 18001.3164, KL Div: 1873.7107\n",
      "Epoch[1/15], Step [190/469], Reconst Loss: 18437.6836, KL Div: 1953.5347\n",
      "Epoch[1/15], Step [200/469], Reconst Loss: 17670.4922, KL Div: 1977.3925\n",
      "Epoch[1/15], Step [210/469], Reconst Loss: 16925.1992, KL Div: 2014.0983\n",
      "Epoch[1/15], Step [220/469], Reconst Loss: 17700.6836, KL Div: 1992.9893\n",
      "Epoch[1/15], Step [230/469], Reconst Loss: 17067.1270, KL Div: 2138.6157\n",
      "Epoch[1/15], Step [240/469], Reconst Loss: 16546.2012, KL Div: 2067.3257\n",
      "Epoch[1/15], Step [250/469], Reconst Loss: 16902.9219, KL Div: 2181.3411\n",
      "Epoch[1/15], Step [260/469], Reconst Loss: 15841.3633, KL Div: 2271.3972\n",
      "Epoch[1/15], Step [270/469], Reconst Loss: 16962.8164, KL Div: 2289.8877\n",
      "Epoch[1/15], Step [280/469], Reconst Loss: 16403.4922, KL Div: 2143.6069\n",
      "Epoch[1/15], Step [290/469], Reconst Loss: 15908.7852, KL Div: 2284.0425\n",
      "Epoch[1/15], Step [300/469], Reconst Loss: 16049.0605, KL Div: 2262.2981\n",
      "Epoch[1/15], Step [310/469], Reconst Loss: 15853.4219, KL Div: 2384.6257\n",
      "Epoch[1/15], Step [320/469], Reconst Loss: 15937.2930, KL Div: 2382.8181\n",
      "Epoch[1/15], Step [330/469], Reconst Loss: 15585.0908, KL Div: 2329.7407\n",
      "Epoch[1/15], Step [340/469], Reconst Loss: 15345.0742, KL Div: 2353.4609\n",
      "Epoch[1/15], Step [350/469], Reconst Loss: 14993.8555, KL Div: 2277.0615\n",
      "Epoch[1/15], Step [360/469], Reconst Loss: 15389.1631, KL Div: 2445.5190\n",
      "Epoch[1/15], Step [370/469], Reconst Loss: 15297.3164, KL Div: 2373.1055\n",
      "Epoch[1/15], Step [380/469], Reconst Loss: 14985.1553, KL Div: 2491.8545\n",
      "Epoch[1/15], Step [390/469], Reconst Loss: 14332.3555, KL Div: 2463.9421\n",
      "Epoch[1/15], Step [400/469], Reconst Loss: 13693.2822, KL Div: 2512.4260\n",
      "Epoch[1/15], Step [410/469], Reconst Loss: 14293.0039, KL Div: 2461.2207\n",
      "Epoch[1/15], Step [420/469], Reconst Loss: 14567.3418, KL Div: 2461.3228\n",
      "Epoch[1/15], Step [430/469], Reconst Loss: 14027.7549, KL Div: 2558.0413\n",
      "Epoch[1/15], Step [440/469], Reconst Loss: 13666.3242, KL Div: 2412.3281\n",
      "Epoch[1/15], Step [450/469], Reconst Loss: 14185.0117, KL Div: 2639.2476\n",
      "Epoch[1/15], Step [460/469], Reconst Loss: 14230.5605, KL Div: 2449.6870\n",
      "Epoch[2/15], Step [10/469], Reconst Loss: 13614.0615, KL Div: 2555.2854\n",
      "Epoch[2/15], Step [20/469], Reconst Loss: 13462.2012, KL Div: 2636.8713\n",
      "Epoch[2/15], Step [30/469], Reconst Loss: 14084.9668, KL Div: 2612.9265\n",
      "Epoch[2/15], Step [40/469], Reconst Loss: 14174.7373, KL Div: 2675.8999\n",
      "Epoch[2/15], Step [50/469], Reconst Loss: 14414.2520, KL Div: 2727.1809\n",
      "Epoch[2/15], Step [60/469], Reconst Loss: 12947.9092, KL Div: 2753.1174\n",
      "Epoch[2/15], Step [70/469], Reconst Loss: 13280.9482, KL Div: 2688.6689\n",
      "Epoch[2/15], Step [80/469], Reconst Loss: 13240.0469, KL Div: 2676.2690\n",
      "Epoch[2/15], Step [90/469], Reconst Loss: 13588.7451, KL Div: 2762.1062\n",
      "Epoch[2/15], Step [100/469], Reconst Loss: 13194.7305, KL Div: 2736.3679\n",
      "Epoch[2/15], Step [110/469], Reconst Loss: 13224.6934, KL Div: 2780.2170\n",
      "Epoch[2/15], Step [120/469], Reconst Loss: 13299.1035, KL Div: 2801.2092\n",
      "Epoch[2/15], Step [130/469], Reconst Loss: 12544.6934, KL Div: 2789.7109\n",
      "Epoch[2/15], Step [140/469], Reconst Loss: 12949.7490, KL Div: 2789.2253\n",
      "Epoch[2/15], Step [150/469], Reconst Loss: 13316.2168, KL Div: 2771.2170\n",
      "Epoch[2/15], Step [160/469], Reconst Loss: 12773.3965, KL Div: 2814.7622\n",
      "Epoch[2/15], Step [170/469], Reconst Loss: 12801.1699, KL Div: 2673.5640\n",
      "Epoch[2/15], Step [180/469], Reconst Loss: 12999.8613, KL Div: 2903.1528\n",
      "Epoch[2/15], Step [190/469], Reconst Loss: 12638.1621, KL Div: 2731.3704\n",
      "Epoch[2/15], Step [200/469], Reconst Loss: 13360.8262, KL Div: 2821.6768\n",
      "Epoch[2/15], Step [210/469], Reconst Loss: 12491.6191, KL Div: 2838.8635\n",
      "Epoch[2/15], Step [220/469], Reconst Loss: 12096.8018, KL Div: 2762.6143\n",
      "Epoch[2/15], Step [230/469], Reconst Loss: 12901.0645, KL Div: 2890.7031\n",
      "Epoch[2/15], Step [240/469], Reconst Loss: 13127.3438, KL Div: 2785.9873\n",
      "Epoch[2/15], Step [250/469], Reconst Loss: 13219.5820, KL Div: 2769.1548\n",
      "Epoch[2/15], Step [260/469], Reconst Loss: 12297.7148, KL Div: 2808.3625\n",
      "Epoch[2/15], Step [270/469], Reconst Loss: 12199.5742, KL Div: 2920.5474\n",
      "Epoch[2/15], Step [280/469], Reconst Loss: 12120.8369, KL Div: 2751.1167\n",
      "Epoch[2/15], Step [290/469], Reconst Loss: 12083.6475, KL Div: 2998.0024\n",
      "Epoch[2/15], Step [300/469], Reconst Loss: 12788.8154, KL Div: 2957.5400\n",
      "Epoch[2/15], Step [310/469], Reconst Loss: 12440.6025, KL Div: 2832.7239\n",
      "Epoch[2/15], Step [320/469], Reconst Loss: 12027.5117, KL Div: 2913.9082\n",
      "Epoch[2/15], Step [330/469], Reconst Loss: 12098.8359, KL Div: 2830.3345\n",
      "Epoch[2/15], Step [340/469], Reconst Loss: 12047.6934, KL Div: 2894.9683\n",
      "Epoch[2/15], Step [350/469], Reconst Loss: 11422.2178, KL Div: 2856.4219\n",
      "Epoch[2/15], Step [360/469], Reconst Loss: 12161.7480, KL Div: 2951.6074\n",
      "Epoch[2/15], Step [370/469], Reconst Loss: 11943.4746, KL Div: 2992.5181\n",
      "Epoch[2/15], Step [380/469], Reconst Loss: 12287.0869, KL Div: 2978.0488\n",
      "Epoch[2/15], Step [390/469], Reconst Loss: 11896.1191, KL Div: 2926.8850\n",
      "Epoch[2/15], Step [400/469], Reconst Loss: 12118.2578, KL Div: 2933.2983\n",
      "Epoch[2/15], Step [410/469], Reconst Loss: 12039.8115, KL Div: 2939.3159\n",
      "Epoch[2/15], Step [420/469], Reconst Loss: 12246.9004, KL Div: 2930.9580\n",
      "Epoch[2/15], Step [430/469], Reconst Loss: 12250.6035, KL Div: 3143.8958\n",
      "Epoch[2/15], Step [440/469], Reconst Loss: 12223.5879, KL Div: 2974.3911\n",
      "Epoch[2/15], Step [450/469], Reconst Loss: 11318.9336, KL Div: 2870.1057\n",
      "Epoch[2/15], Step [460/469], Reconst Loss: 12077.5215, KL Div: 2986.7976\n",
      "Epoch[3/15], Step [10/469], Reconst Loss: 11807.3184, KL Div: 3115.5046\n",
      "Epoch[3/15], Step [20/469], Reconst Loss: 11887.1064, KL Div: 2957.5376\n",
      "Epoch[3/15], Step [30/469], Reconst Loss: 11677.0195, KL Div: 2951.9692\n",
      "Epoch[3/15], Step [40/469], Reconst Loss: 12420.5439, KL Div: 3044.1777\n",
      "Epoch[3/15], Step [50/469], Reconst Loss: 11149.3340, KL Div: 2992.4172\n",
      "Epoch[3/15], Step [60/469], Reconst Loss: 12205.2695, KL Div: 2905.1958\n",
      "Epoch[3/15], Step [70/469], Reconst Loss: 11517.0371, KL Div: 3024.2849\n",
      "Epoch[3/15], Step [80/469], Reconst Loss: 12128.1016, KL Div: 2878.7820\n",
      "Epoch[3/15], Step [90/469], Reconst Loss: 11548.9297, KL Div: 3077.7708\n",
      "Epoch[3/15], Step [100/469], Reconst Loss: 11842.6689, KL Div: 2941.9917\n",
      "Epoch[3/15], Step [110/469], Reconst Loss: 11431.4023, KL Div: 3014.5588\n",
      "Epoch[3/15], Step [120/469], Reconst Loss: 11150.7861, KL Div: 2920.9343\n",
      "Epoch[3/15], Step [130/469], Reconst Loss: 11517.9209, KL Div: 3141.7305\n",
      "Epoch[3/15], Step [140/469], Reconst Loss: 12158.9854, KL Div: 2997.3347\n",
      "Epoch[3/15], Step [150/469], Reconst Loss: 11644.2148, KL Div: 3004.1975\n",
      "Epoch[3/15], Step [160/469], Reconst Loss: 11957.2246, KL Div: 3035.1348\n",
      "Epoch[3/15], Step [170/469], Reconst Loss: 12073.9541, KL Div: 3010.8220\n",
      "Epoch[3/15], Step [180/469], Reconst Loss: 11971.0684, KL Div: 3025.8096\n",
      "Epoch[3/15], Step [190/469], Reconst Loss: 11522.8027, KL Div: 3132.2573\n",
      "Epoch[3/15], Step [200/469], Reconst Loss: 11793.3262, KL Div: 2922.1572\n",
      "Epoch[3/15], Step [210/469], Reconst Loss: 11221.4023, KL Div: 2994.8936\n",
      "Epoch[3/15], Step [220/469], Reconst Loss: 11102.2012, KL Div: 2950.5417\n",
      "Epoch[3/15], Step [230/469], Reconst Loss: 11639.7949, KL Div: 3035.2637\n",
      "Epoch[3/15], Step [240/469], Reconst Loss: 11366.9951, KL Div: 2911.1025\n",
      "Epoch[3/15], Step [250/469], Reconst Loss: 12076.2773, KL Div: 3150.7627\n",
      "Epoch[3/15], Step [260/469], Reconst Loss: 12032.9531, KL Div: 3028.2019\n",
      "Epoch[3/15], Step [270/469], Reconst Loss: 11507.0215, KL Div: 3122.5754\n",
      "Epoch[3/15], Step [280/469], Reconst Loss: 11620.1709, KL Div: 2917.7156\n",
      "Epoch[3/15], Step [290/469], Reconst Loss: 11710.0410, KL Div: 3007.3337\n",
      "Epoch[3/15], Step [300/469], Reconst Loss: 11579.1348, KL Div: 2915.7583\n",
      "Epoch[3/15], Step [310/469], Reconst Loss: 12047.7109, KL Div: 3028.1716\n",
      "Epoch[3/15], Step [320/469], Reconst Loss: 11652.7832, KL Div: 2999.9412\n",
      "Epoch[3/15], Step [330/469], Reconst Loss: 11042.6465, KL Div: 3001.1992\n",
      "Epoch[3/15], Step [340/469], Reconst Loss: 11592.2520, KL Div: 3049.4346\n",
      "Epoch[3/15], Step [350/469], Reconst Loss: 11128.1445, KL Div: 3026.6018\n",
      "Epoch[3/15], Step [360/469], Reconst Loss: 11682.4004, KL Div: 3057.1580\n",
      "Epoch[3/15], Step [370/469], Reconst Loss: 11821.9531, KL Div: 3033.0225\n",
      "Epoch[3/15], Step [380/469], Reconst Loss: 11181.1113, KL Div: 3096.3086\n",
      "Epoch[3/15], Step [390/469], Reconst Loss: 11654.9199, KL Div: 3068.5325\n",
      "Epoch[3/15], Step [400/469], Reconst Loss: 11905.9629, KL Div: 3082.6785\n",
      "Epoch[3/15], Step [410/469], Reconst Loss: 11242.0762, KL Div: 3041.1448\n",
      "Epoch[3/15], Step [420/469], Reconst Loss: 11875.4111, KL Div: 3036.8411\n",
      "Epoch[3/15], Step [430/469], Reconst Loss: 10986.1328, KL Div: 3191.2317\n",
      "Epoch[3/15], Step [440/469], Reconst Loss: 11738.7393, KL Div: 3132.7263\n",
      "Epoch[3/15], Step [450/469], Reconst Loss: 11412.1006, KL Div: 3045.3713\n",
      "Epoch[3/15], Step [460/469], Reconst Loss: 11095.2705, KL Div: 3124.2642\n",
      "Epoch[4/15], Step [10/469], Reconst Loss: 11319.1777, KL Div: 3043.7571\n",
      "Epoch[4/15], Step [20/469], Reconst Loss: 11061.2910, KL Div: 3131.0298\n",
      "Epoch[4/15], Step [30/469], Reconst Loss: 11845.7393, KL Div: 3191.6885\n",
      "Epoch[4/15], Step [40/469], Reconst Loss: 11123.7695, KL Div: 2982.6597\n",
      "Epoch[4/15], Step [50/469], Reconst Loss: 10913.6699, KL Div: 2994.3613\n",
      "Epoch[4/15], Step [60/469], Reconst Loss: 11026.5781, KL Div: 3038.0828\n",
      "Epoch[4/15], Step [70/469], Reconst Loss: 11505.4424, KL Div: 3038.0459\n",
      "Epoch[4/15], Step [80/469], Reconst Loss: 11229.3330, KL Div: 3200.9243\n",
      "Epoch[4/15], Step [90/469], Reconst Loss: 11652.0400, KL Div: 3117.5586\n",
      "Epoch[4/15], Step [100/469], Reconst Loss: 11194.7061, KL Div: 3138.9460\n",
      "Epoch[4/15], Step [110/469], Reconst Loss: 11847.3965, KL Div: 3059.3293\n",
      "Epoch[4/15], Step [120/469], Reconst Loss: 10964.3203, KL Div: 3042.1631\n",
      "Epoch[4/15], Step [130/469], Reconst Loss: 11160.0986, KL Div: 3137.7239\n",
      "Epoch[4/15], Step [140/469], Reconst Loss: 11315.7979, KL Div: 2973.8311\n",
      "Epoch[4/15], Step [150/469], Reconst Loss: 11120.2793, KL Div: 3170.3354\n",
      "Epoch[4/15], Step [160/469], Reconst Loss: 11723.5859, KL Div: 3064.7498\n",
      "Epoch[4/15], Step [170/469], Reconst Loss: 11621.3730, KL Div: 3128.6558\n",
      "Epoch[4/15], Step [180/469], Reconst Loss: 10999.9473, KL Div: 3202.5161\n",
      "Epoch[4/15], Step [190/469], Reconst Loss: 11964.5000, KL Div: 3160.0698\n",
      "Epoch[4/15], Step [200/469], Reconst Loss: 10863.9785, KL Div: 3169.3376\n",
      "Epoch[4/15], Step [210/469], Reconst Loss: 10992.3281, KL Div: 3093.2258\n",
      "Epoch[4/15], Step [220/469], Reconst Loss: 10887.7461, KL Div: 3078.5515\n",
      "Epoch[4/15], Step [230/469], Reconst Loss: 10862.9434, KL Div: 3121.7769\n",
      "Epoch[4/15], Step [240/469], Reconst Loss: 11392.8691, KL Div: 3131.7249\n",
      "Epoch[4/15], Step [250/469], Reconst Loss: 11245.0371, KL Div: 3065.2427\n",
      "Epoch[4/15], Step [260/469], Reconst Loss: 11344.7305, KL Div: 3198.7332\n",
      "Epoch[4/15], Step [270/469], Reconst Loss: 10773.8984, KL Div: 3074.2617\n",
      "Epoch[4/15], Step [280/469], Reconst Loss: 11195.2266, KL Div: 3113.4343\n",
      "Epoch[4/15], Step [290/469], Reconst Loss: 11595.4180, KL Div: 3091.0786\n",
      "Epoch[4/15], Step [300/469], Reconst Loss: 11559.9121, KL Div: 3183.3115\n",
      "Epoch[4/15], Step [310/469], Reconst Loss: 11318.8555, KL Div: 3031.5886\n",
      "Epoch[4/15], Step [320/469], Reconst Loss: 11421.7949, KL Div: 3103.5361\n",
      "Epoch[4/15], Step [330/469], Reconst Loss: 10744.2734, KL Div: 3081.5164\n",
      "Epoch[4/15], Step [340/469], Reconst Loss: 11368.0000, KL Div: 3133.0518\n",
      "Epoch[4/15], Step [350/469], Reconst Loss: 11347.1875, KL Div: 3224.5454\n",
      "Epoch[4/15], Step [360/469], Reconst Loss: 11596.7686, KL Div: 3197.6804\n",
      "Epoch[4/15], Step [370/469], Reconst Loss: 10888.9102, KL Div: 3023.2180\n",
      "Epoch[4/15], Step [380/469], Reconst Loss: 11205.8115, KL Div: 3364.3506\n",
      "Epoch[4/15], Step [390/469], Reconst Loss: 10702.1699, KL Div: 3008.0520\n",
      "Epoch[4/15], Step [400/469], Reconst Loss: 11249.2412, KL Div: 3133.1777\n",
      "Epoch[4/15], Step [410/469], Reconst Loss: 10645.6328, KL Div: 3054.7212\n",
      "Epoch[4/15], Step [420/469], Reconst Loss: 11328.5371, KL Div: 3104.9231\n",
      "Epoch[4/15], Step [430/469], Reconst Loss: 10889.4082, KL Div: 3174.0842\n",
      "Epoch[4/15], Step [440/469], Reconst Loss: 11102.2188, KL Div: 3139.5708\n",
      "Epoch[4/15], Step [450/469], Reconst Loss: 11463.6523, KL Div: 3251.0154\n",
      "Epoch[4/15], Step [460/469], Reconst Loss: 11260.4004, KL Div: 3126.5144\n",
      "Epoch[5/15], Step [10/469], Reconst Loss: 11238.6211, KL Div: 3239.9509\n",
      "Epoch[5/15], Step [20/469], Reconst Loss: 11699.1973, KL Div: 3130.4285\n",
      "Epoch[5/15], Step [30/469], Reconst Loss: 10998.7490, KL Div: 3128.0461\n",
      "Epoch[5/15], Step [40/469], Reconst Loss: 11083.2822, KL Div: 3092.7217\n",
      "Epoch[5/15], Step [50/469], Reconst Loss: 10232.4082, KL Div: 3040.8840\n",
      "Epoch[5/15], Step [60/469], Reconst Loss: 10318.9961, KL Div: 3204.0161\n",
      "Epoch[5/15], Step [70/469], Reconst Loss: 11378.9941, KL Div: 3303.6494\n",
      "Epoch[5/15], Step [80/469], Reconst Loss: 10938.4902, KL Div: 3105.4155\n",
      "Epoch[5/15], Step [90/469], Reconst Loss: 10925.3945, KL Div: 3238.1523\n",
      "Epoch[5/15], Step [100/469], Reconst Loss: 10818.1699, KL Div: 3230.5950\n",
      "Epoch[5/15], Step [110/469], Reconst Loss: 11081.5156, KL Div: 3126.0703\n",
      "Epoch[5/15], Step [120/469], Reconst Loss: 11256.1094, KL Div: 3223.4961\n",
      "Epoch[5/15], Step [130/469], Reconst Loss: 10711.7930, KL Div: 3128.6030\n",
      "Epoch[5/15], Step [140/469], Reconst Loss: 10679.1211, KL Div: 3205.4092\n",
      "Epoch[5/15], Step [150/469], Reconst Loss: 10747.9141, KL Div: 3109.0754\n",
      "Epoch[5/15], Step [160/469], Reconst Loss: 10780.6465, KL Div: 3071.2170\n",
      "Epoch[5/15], Step [170/469], Reconst Loss: 11896.0879, KL Div: 3274.4575\n",
      "Epoch[5/15], Step [180/469], Reconst Loss: 10967.0977, KL Div: 3030.7551\n",
      "Epoch[5/15], Step [190/469], Reconst Loss: 11173.7188, KL Div: 3258.2871\n",
      "Epoch[5/15], Step [200/469], Reconst Loss: 10581.2012, KL Div: 3180.4099\n",
      "Epoch[5/15], Step [210/469], Reconst Loss: 10760.9531, KL Div: 3115.5493\n",
      "Epoch[5/15], Step [220/469], Reconst Loss: 11131.3125, KL Div: 3129.7402\n",
      "Epoch[5/15], Step [230/469], Reconst Loss: 11070.3633, KL Div: 3125.5249\n",
      "Epoch[5/15], Step [240/469], Reconst Loss: 10612.7939, KL Div: 3181.7334\n",
      "Epoch[5/15], Step [250/469], Reconst Loss: 10817.1133, KL Div: 3084.5398\n",
      "Epoch[5/15], Step [260/469], Reconst Loss: 10872.5410, KL Div: 3151.5532\n",
      "Epoch[5/15], Step [270/469], Reconst Loss: 10851.0781, KL Div: 3107.0889\n",
      "Epoch[5/15], Step [280/469], Reconst Loss: 11173.2617, KL Div: 3350.9036\n",
      "Epoch[5/15], Step [290/469], Reconst Loss: 10481.2070, KL Div: 3069.9951\n",
      "Epoch[5/15], Step [300/469], Reconst Loss: 10505.0879, KL Div: 3097.9666\n",
      "Epoch[5/15], Step [310/469], Reconst Loss: 10778.5752, KL Div: 3102.2520\n",
      "Epoch[5/15], Step [320/469], Reconst Loss: 11181.7148, KL Div: 3118.4209\n",
      "Epoch[5/15], Step [330/469], Reconst Loss: 10649.1113, KL Div: 3285.9543\n",
      "Epoch[5/15], Step [340/469], Reconst Loss: 10515.7285, KL Div: 3116.9109\n",
      "Epoch[5/15], Step [350/469], Reconst Loss: 10938.2012, KL Div: 3135.6829\n",
      "Epoch[5/15], Step [360/469], Reconst Loss: 11466.6133, KL Div: 3166.4167\n",
      "Epoch[5/15], Step [370/469], Reconst Loss: 11169.6885, KL Div: 3131.5400\n",
      "Epoch[5/15], Step [380/469], Reconst Loss: 10694.1758, KL Div: 3186.5725\n",
      "Epoch[5/15], Step [390/469], Reconst Loss: 10792.7617, KL Div: 3128.0698\n",
      "Epoch[5/15], Step [400/469], Reconst Loss: 10707.6289, KL Div: 3205.7219\n",
      "Epoch[5/15], Step [410/469], Reconst Loss: 10853.3672, KL Div: 3137.0103\n",
      "Epoch[5/15], Step [420/469], Reconst Loss: 10167.1768, KL Div: 3182.1445\n",
      "Epoch[5/15], Step [430/469], Reconst Loss: 10797.9023, KL Div: 3044.6760\n",
      "Epoch[5/15], Step [440/469], Reconst Loss: 11199.2383, KL Div: 3150.0771\n",
      "Epoch[5/15], Step [450/469], Reconst Loss: 10687.6250, KL Div: 3166.3799\n",
      "Epoch[5/15], Step [460/469], Reconst Loss: 10932.6719, KL Div: 3222.8110\n",
      "Epoch[6/15], Step [10/469], Reconst Loss: 11287.1963, KL Div: 3252.2366\n",
      "Epoch[6/15], Step [20/469], Reconst Loss: 10635.4551, KL Div: 3144.5928\n",
      "Epoch[6/15], Step [30/469], Reconst Loss: 11275.5215, KL Div: 3169.1648\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1,image_size)\n",
    "        x_reconst,mu,log_var = model(x)\n",
    "        # Compute reconstruction loss and kl divergence\n",
    "        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst,x,size_average=False)\n",
    "        kl_div = -0.5 * torch.sum(1 + log_var-mu.pow(2)-log_var.exp())\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "    with torch.no_grad():\n",
    "        # Save the sampled images\n",
    "        z = torch.randn(batch_size,z_dim).to(device)\n",
    "        out = model.decode(z).view(-1,1,28,28)\n",
    "        save_image(out,os.path.join(sample_dir,'sampled-{}.png'.format(epoch+1)))\n",
    "        \n",
    "        # Save the reconstructed images\n",
    "        out,_,_ = model(x)\n",
    "        x_concat = torch.cat([x.view(-1,1,28,28),out.view(-1,1,28,28)],dim=3)\n",
    "        save_image(x_concat,os.path.join(sample_dir,'reconst-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
